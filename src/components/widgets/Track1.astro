---
import type { Brands as Props } from '~/types';
import Headline from '~/components/ui/Headline.astro';
import Button from '../ui/Button.astro';
import { Icon } from 'astro-icon/components';
const { title = '', subtitle = '', tagline = '', id } = Astro.props;
---

<div class="bg-blue-50 mt-8 pb-8">
  <div class="max-w-7xl mx-auto pt-8" id={id}>
    <Headline title={title} subtitle={subtitle} tagline={tagline} />
    <div class="flex gap-8 justify-center">
      <Button variant="secondary" href={'https://github.com/OpenSafetyLab/mllm_challenge'}>
        <span class="mr-2">{'Issues'}</span>
        <Icon name="tabler:chevron-right" class="w-6 h-6" />
      </Button><Button variant="secondary">
        <span class="mr-2">{'Submission'}</span>
        <Icon name="tabler:ban" class="w-6 h-6" />
      </Button>
    </div>

    <!-- <h2 class="text-center text-2xl md:text-3xl py-8">Leaderboard</h2>

  <div class="px-4 sm:px-6 lg:px-8">
    <div class="sm:flex sm:items-center">
      <div class="sm:flex-auto">
        <li>Participating Teams: 52</li>
        <li>Countries and Regions: 11</li>
        <li>Submissions: 600+</li>
        <p class="mt-2 text-sm text-gray-700">
          This track was characterized by the greatest diversity among this challenge, with participation from teams
          representing up to 11 countries and regions. The difference in scores between the top 5 teams was less than 10
          points.
        </p>
      </div>
    </div>
    <div class="my-8 flow-root">
      <div class="-mx-4 -my-2 overflow-x-auto sm:-mx-6 lg:-mx-8">
        <div class="inline-block min-w-full py-2 align-middle sm:px-6 lg:px-8">
          <div class="overflow-hidden shadow ring-1 ring-black ring-opacity-5 sm:rounded-lg">
            <table class="min-w-full divide-y divide-gray-300">
              <thead class="bg-gray-50">
                <tr>
                  <th scope="col" class="py-3.5 pl-4 pr-3 text-left text-sm font-semibold text-gray-900 sm:pl-6">
                    Name
                  </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> Rank </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900">
                    Country / Region
                  </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> Institution </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900">
                    Overall Score (primary)
                  </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> Team Name </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> CH1 Score </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> CH2 Score </th>
                  <th scope="col" class="px-3 py-3.5 text-left text-sm font-semibold text-gray-900"> CH3 Score</th>
                </tr>
              </thead>
              <tbody class="divide-y divide-gray-200 bg-white" id="track1-table-body"> </tbody>
            </table>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div id="track1-pagination" class="my-4 flex justify-center items-center"></div> -->
    <h2 class="text-center text-2xl md:text-3xl py-8">Introduction</h2>
    <p class="text-gray-700">
      <!-- Multimodal large language models (MLLMs), which are capable of processing images and performing visual
      question-and-answer tasks, may generate harmful content if the input image or text contains inducing or harmful
      properties. Furthermore, the introduction of perturbations to images or texts can degrade the performance of these
      models in various visual tasks, such as object counting and reasoning. This necessitates an investigation into the
      effects of different input perturbations or attack methods on the performance and trustworthiness of MLLMs.
      Understanding these impacts is crucial for developing strategies to mitigate potential risks associated with
      MLLMs. -->
      The primary goal of this challenge is to execute a successful attack on a MLLM, Llava-1.5. Participants must alter
      either the input image or text to significantly impair the model's accuracy. The core of this challenge involves ingeniously
      designing inputs that prompt the MLLM to generate incorrect or harmful outputs, thus evaluating the model's robustness
      against attacks. 
    </p>
    <br>
    <p>
      Generally, for each given input pair (I, T), we aim to design specific MLLM attack methods to automatically
      construct image adversarial perturbation ΔI or a textual prompt ΔT, such that the target MLLM may generate inaccurate
      or unsafe outputs (i.e., choice for multiple-choice questions and sentences for harmlessness evaluation) with prompted
      inputs (i.e., (I+ΔI, T) / (I, T+ΔT) / (I+ΔI, T) / (I+ΔI,T+ΔT)). The similarity between the origin and the adversarial
      should be larger than 0.9. The lower accuracy or more unsafe responses indicates the better attack methods.
    </p>
    <!-- <h2 class="text-center text-2xl md:text-3xl py-8">Participation</h2>
    <p class="text-gray-700">Participants need to provide submission file which contains query and image.</p>
    <p class="text-gray-700 mt-4">
      Submissions will be evaluated based on the following criteria:

      <li>
        <strong>Choice Accuracy: </strong>The effectiveness of the attack will be measured by the decrease in the
        model's ability to correctly interpret and respond to the modified inputs.
      </li><li>
        <strong>Answer Safety: </strong>The challenge also involves ensuring that the model's responses to adversarial
        inputs do not generate harmful, biased, or inappropriate content.
      </li><li>
        <strong>Adversarial Input Similarity: </strong>A key aspect of the challenge is maintaining a semblance of
        similarity between the original and modified inputs. This ensures that the attack remains subtle and tests the
        model's ability to detect nuanced alterations.
      </li>
    </p> -->
    <!-- <h2 class="text-center text-2xl md:text-3xl py-8">Award</h2>
    <div class="flex justify-center">
      <table class="bg-white">
        <tbody>
          <tr>
            <td class="w-64 border px-4 py-2">Outstanding Champion</td>
            <td class="w-64 border px-4 py-2">USD $15,000</td>
          </tr>
          <tr class="bg-gray-50">
            <td class="border px-4 py-2">Honorable Runner-up</td>
            <td class="border px-4 py-2">USD $5,000</td>
          </tr>
          <tr class="bg-gray-50">
            <td class="border px-4 py-2">Innovation Award</td>
            <td class="border px-4 py-2">USD $5,000</td>
          </tr>
        </tbody>
      </table>
    </div> -->
    <div class="flex gap-8 justify-center mt-4">
      <Button variant="secondary" href={'/challenges/track1'}>
        <span>Check Details</span>
      </Button>
    </div>
    <!-- <h2 class="text-center text-2xl md:text-3xl py-8">Contact</h2>
    <li>
      <a href="https://github.com/OpenSafetyLab/mllm_challenge/issues" class="hover:underline text-sky-600 font-medium"
        >GitHub issue</a
      >
    </li>
    <h2 class="text-center text-2xl md:text-3xl py-8">Related Literature</h2>
    <li>
      <a href="https://arxiv.org/abs/2403.17830" class="hover:underline text-sky-600 font-medium"
        >Assessment of Multimodal Large Language Models in Alignment with Human Values</a
      >
    </li>
    <li>
      <a href="https://arxiv.org/abs/2402.05044" class="hover:underline text-sky-600 font-medium"
        >SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models</a
      >
    </li> -->
  </div>
</div>

<script>
  const track1Data = [
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    {
      rank: 'Lindsay',
      country: 'Developer',
      institution: 'lindsay',
      score: 'Member',
      teamName: 'asdf',
      ch1Score: 'asdf',
      ch2Score: 'asdf',
      ch3Score: '12',
      name: 'asd',
    },
    // More track1Data...
  ];

  const pageSize = 10;
  let currentPage = 1;

  function padtrack1Data(track1Data) {
    const remainder = track1Data.length % pageSize;
    if (remainder !== 0) {
      const padCount = pageSize - remainder;
      for (let i = 0; i < padCount; i++) {
        track1Data.push({
          rank: '',
          country: '',
          institution: '',
          score: '',
          teamName: '',
          ch1Score: '',
          ch2Score: '',
          ch3Score: '',
          name: '&nbsp;',
        });
      }
    }
  }

  function renderTable(page) {
    const startIndex = (page - 1) * pageSize;
    const endIndex = Math.min(startIndex + pageSize, track1Data.length);
    const tableBody = document.getElementById('track1-table-body');
    if (!tableBody) return;
    tableBody.innerHTML = '';
    for (let i = startIndex; i < endIndex; i++) {
      const team = track1Data[i];
      const row = document.createElement('tr');
      row.innerHTML = `
        <td class="whitespace-nowrap py-4 pl-4 pr-3 text-sm font-medium text-gray-900 sm:pl-6">${team.name}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.rank}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.country}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.institution}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.score}</td>
        <td class="whitespace-nowrap py-4 pl-4 pr-3 text-sm font-medium text-gray-900 sm:pl-6">${team.teamName}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.ch1Score}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.ch2Score}</td>
        <td class="whitespace-nowrap px-3 py-4 text-sm text-gray-500">${team.ch3Score}</td>
      `;
      tableBody.appendChild(row);
    }
  }

  function renderPagination() {
    const totalPages = Math.ceil(track1Data.length / pageSize);
    const pagination = document.getElementById('track1-pagination');
    if (!pagination) return;
    pagination.innerHTML = '';
    for (let i = 1; i <= totalPages; i++) {
      const button = document.createElement('button');
      button.textContent = i.toString();
      button.classList.add('px-3', 'py-1', 'mr-2', 'border', 'rounded', 'bg-white', 'hover:bg-gray-100');
      if (i === currentPage) {
        button.classList.add('font-bold');
      }
      button.addEventListener('click', () => {
        currentPage = i;
        renderTable(currentPage);
        renderPagination();
      });
      pagination.appendChild(button);
    }
  }
  padtrack1Data(track1Data);
  renderTable(currentPage);
  renderPagination();
</script>
