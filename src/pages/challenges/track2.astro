---
import Header from '~/components/widgets/Header.astro';
import Hero from '~/components/widgets/Hero.astro';
import Content from '~/components/widgets/Content.astro';
// import Timeline from '~/components/ui/Timeline.astro';
import FAQs from '~/components/widgets/FAQs.astro';
// import Brands from '~/components/widgets/Brands.astro';
// import Timezone from '~/components/widgets/Timezone.astro';
import Layout from '~/layouts/PageLayout.astro';
import Testimonials from '~/components/widgets/Testimonials.astro';
import Track2 from '~/components/widgets/Track2.astro';
import Track3 from '~/components/widgets/Track3.astro';
import Track1 from '~/components/widgets/Track1.astro';
import { getPermalink } from '~/utils/permalinks';
// import Sponsor from '~/components/widgets/Sponsor.astro';

const metadata = {
  title: 'Challenges',
};

const headerData = {
  links: [
    {
      text: 'TRACK1',
      href: getPermalink('/challenges/track1'),
    },
    // {
    //   text: 'TRACK2',
    //   href: getPermalink('/challenges/track2'),
    // },
    {
      text: 'TRACK2',
      href: getPermalink('/challenges/track2'),
    },
    {
      text: 'Home',
      href: getPermalink('/'),
    },
  ],
};
---

<Layout metadata={metadata}>
  <Fragment slot="header">
    <Header {...headerData} isSticky />
  </Fragment>

  <!-- Hero2 Widget ******************* -->

  <Hero tagline="ICML 2024 Workshop">
    <Fragment slot="title">Track2 - Agent trustworthy</Fragment>
    <Fragment slot="subtitle"
      >Future Frontiers in Trustworthy Foundation Model Agents: Environments, Benchmarks, and Solutions
    </Fragment>
  </Hero>

  <!-- Content Widget **************** -->

  <Content id="Overview" classes={{ container: 'pb-0 md:pb-0 lg:pb-0' }}>
    <!-- <Fragment slot="bg">
        <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment> -->
    <Fragment slot="title"> Introduction </Fragment>
    <Fragment slot="content">
      <p>
        The evolution of Artificial Intelligence (AI) reflects its increasing capability and integration into our daily
        lives, from basic automated tools to sophisticated, autonomous systems. Initially, AI systems were simple agents
        performing goal-directed actions without specific human commands. Over time, these evolved into Multimodal Large
        Language Models (MLLMs, e.g., GPT-4 / GPT-4o and Gemini), which not only execute complex tasks but also enhance
        decision-making through advanced language understanding and generation capabilities. The most advanced tier of
        AI development includes ethically aligned, trustworthy agents. These agents are designed to operate reliably
        within ethical and safety frameworks, illustrating both the benefits and the essential need to manage the risks
        associated with AI's integration into society. This tiered framework—from basic agents, through linguistically
        skilled LLMs and MLLMs, to ethically guided trustworthy agents—underscores the progression and potential of AI
        in enhancing human capabilities and addressing complex challenges.
      </p>
      <p>
        The trustworthiness of agents is a brand new field, significantly different from the traditional trustworthiness
        of large language models (LLMs). This is mainly reflected in the following areas:
      </p>
      <ul>
        <li>
          <span class="font-bold">More complex environments: </span>
          Agents can operate autonomously, utilizing tools [2, 3] or interacting with their environment [4]. For instance,
          scenarios involving tool use may incorporate extensive external network or database information. In embodied scenarios,
          agents interact in real-time with real or simulated environments. Traditional evaluations of trustworthy safety
          in Large Language Models (LLMs) typically focus only on input and output, hence, a more comprehensive assessment
          method is necessary to evaluate the safety of agents.
        </li>
        <li>
          <span class="font-bold">More diverse benchmarks for trustworthy AI agent evaluation: </span>
          The more complex macro-architecture of multi-modal AI agents indicates that the trustworthy evaluation for agents
          should be revised. For instance, a comprehensive UI-assistant agent safety benchmark should evaluate both the safety
          capability of each module (controller or planner) inside the agent against adversarial attack hidden in the website
          or UI of applications, but also evaluate the safety in the collaboration among all modules against the whole attack
          procedure. Therefore, more advanced evaluation metrics are also worthy to explore. Traditional evaluations of trustworthiness
          and safety in Multi-modal Foundation Models (MFMs) typically focus only on input and output, hence, a more comprehensive
          assessment method is necessary to evaluate the safety of agents [5].
        </li>
        <li>
          <span class="font-bold">More complex agent systems:</span>
          The current popular mode of agent interaction involves agent systems [1-3], where multiple agents collaborate.
          This differs significantly from the traditional single-point security evaluations of LLMs in terms of trustworthiness.
          We need to consider not only the trustworthiness of individual agents within a group but also the overall system’s
          trustworthiness. Therefore, a more comprehensive and suitable evaluation framework for the trustworthiness of agent
          systems is required.
        </li>
        <li>
          <span class="font-bold">New position papers retarding trustworthy agents:</span>
          Above difference has revealed that trustworthiness in AI agents are largely different from vanilla MFM trustworthy
          assessment. Meanwhile, as the evolution of AI agent research and social environment, more novel safety threats
          will be revealed gradually [5]. Therefore, we also strongly encourage the authors to submit new position papers
          to rethink current trustworthy AI agent areas, and provide constructive or insightful high-level suggestions to
          improve the trustworthiness in AI agents.
        </li>
      </ul>
      <div class="text-sm">
        <!-- 添加了Tailwind的类来设置整体字体大小 -->
        <ul>
          <li>[1] CAMEL: Communicative Agents for "Mind" Exploration of Large Language Model Society</li>
          <li>[2] MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</li>
          <li>[3] AutoGPT: <a href="https://github.com/Significant-Gravitas/AutoGPT" class="underline">AutoGPT</a></li>
          <li>[4] RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</li>
          <li>
            [5] Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast. <a
              href="https://arxiv.org/abs/2402.08567"
              class="underline">arXiv preprint arXiv:2402.08567</a
            >.
          </li>
          <li>[6] PaLM-E: An Embodied Multimodal Language Model</li>
          <li>[7] Octopus: Embodied Vision-Language Programmer from Environmental Feedback</li>
        </ul>
      </div>
    </Fragment>
  </Content>

  <Content>
    <Fragment slot="title"> General Guideline</Fragment>
    <Fragment slot="content">
      <h1 class="text-center font-bold text-4xl">Why Organize This Track?</h1>
      <br />
      <p>
        Different from trustworthy evaluation for unique multi-modal foundation models (MFMs, including but not limited
        to LLMs), trustworthy investigation for AI agents should focus on not only the safety / factuality / robustness
        of backend foundation models, but also reliable and truthful interaction within the whole agent systems (among
        different modules or multiple agents). Nowadays, the investigation in trustworthiness for AI agents (especially
        for multi-modal agents) is only in the early exploration. The available experimental environments and benchmarks
        for agent trustworthiness are still insufficient, which restricts the exploration of more advanced trustworthy
        AI agent learning or inference frameworks. Furthermore, the key questions and potential threats in trustworthy
        AI agents are still worthy to be clarified. Clearer questions definition and forecasting for existing and future
        safety threats in AI agents are also significant for future research.

        <p>
          Therefore we aim to organize this track, aiming to encourage researchers to provide constructive solutions or
          insightful perspectives regarding trustworthy AI agents.
        </p>
      </p>
      <br />
      <h1 class="text-center font-bold text-4xl">Topic</h1>
      <br />
      <p class="font-bold">For research areas, supported topics are listed (but not limited to) as follows:</p>
      <p>- Adversarial attack and defense, poisoning, hijacking and security for AI agents.</p>
      <p>- Technical approaches to privacy, fairness, accountability and regulation for trustworthy agents.</p>
      <p>- Transparency, interpretability and monitoring for trustworthy agents.</p>
      <p>- Truthfulness, factuality, honesty and sycophancy of AI agents.</p>
      <p>- Novel safety challenge with trustworthy AI agents.</p>
      <p>- Realistic application for trustworthy AI agents.</p>
      <p>- Trustworthy AI agents in social science.</p>
      <p class="font-bold">For research work types, supported types are listed (but not limited to) as follows:</p>
      <p>
        - New AI agent environments / platforms / playgrounds for trustworthy agent research, simulation, and
        application.
      </p>
      <p>- New AI agent trustworthy benchmarks for evaluation of existing or future trustworthy agents.</p>
      <p>- New trustworthy agent frameworks for practical applications.</p>
      <p>
        - Insightful position paper to rethink critical issues and future directions regarding trustworthiness in AI
        agents.
      </p>
      <p>
        - Other related types to present your solutions or opinions regarding trustworthy AI agents are also feasible.
      </p>
      <br />
      <h1 class="text-center font-bold text-4xl">Example Ideas</h1>
      <br />
      <p>
        In order to provide clearer guidance for submissions, we demonstrate some examples regarding the following
        topics:
      </p>
      <p>- A large-scale multi-agent playground / environment for social communication simulation.</p>
      <p>- A comprehensive trustworthy benchmark for embodied AI agents.</p>
      <p>- Adversarial attack and corresponding defense methods for UI-assistant agents.</p>
      <p>- Revealing new safety threats in multi-agent systems.</p>
      <br />
      <h1 class="text-center font-bold text-4xl">Stage</h1>
      <br />
      <h2 class="font-bold text-2xl">Stage1</h2>
      <p>Submit your proposal in the form of a workshop paper. It will be reviewed by an expert committee.</p>
      <h2 class="font-bold text-2xl">Stage2</h2>
      <p>
        We will select the top 5 outstanding workshop proposals as oral workshop papers and provide the winning prizes.
      </p>
      <h2 class="font-bold text-2xl">Stage3</h2>
      <p>
        For outstanding proposals, we will provide essential support and collaboration to create new methodologies
        proposed in the submissions.
      </p>
      <br />
      <h1 class="text-center font-bold text-4xl">Essential Rules</h1>
      <br />
      <p>
        1. All proposals are judged via double-blind peer-review. If one submission leaks the authors information during
        review phase, this submission will be desk rejected.
      </p>
      <p>
        2. Each proposal will be evaluated by the judges according to the criteria outlined below. Prizes will be
        awarded to the proposals which score the best according to the aggregate evaluations of the judges.
      </p>
      <p>
        3. For proposals regarding trustworthy agent environment and benchmarks, if the submission proposes a simulated
        environment or a dataset, pay attention to the legal aspects of data sourcing to protect the copyright. It's
        acceptable and recommended to use data that is already freely available; however, make sure that obtaining the
        data complies with the licensing or usage guidelines set by its originator.
      </p>
      <p>
        4.You are eligible to submit as an individual, on behalf of an organization, from a for-profit or a
        not-for-profit - we are impartial as to your affiliation (or lack thereof).
      </p>
      <p>
        5.Prizes will be distributed evenly to named lead authors on the paper, unless other instructions are provided.
      </p>
      <br />
      <h1 class="text-center font-bold text-4xl">How Will The Submissions Be Evaluated</h1>
      <br />
      <div class="flex justify-center">
        <table class="bg-white">
          <tbody>
            <tr>
              <td class="w-128 border px-4 py-2">Clearly illustrate the motivation. </td>
              <td class="w-128 border px-4 py-2"
                >Your submission should emphasize corresponding motivation, i.e., which limitation affects the
                trustworthiness of AI agents, or what is the most critical issue to restrict the trustworthiness of AI
                agents.
              </td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2"
                >Highlight the significance of proposed platform / benchmark / training and inference framework / novel
                opinions.
              </td>
              <td class="border px-4 py-2"
                >We recommend the authors to highlight the significance (include but not limited to novelty) of proposed
                ideas and methods.
              </td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2">Fine-grained.</td>
              <td class="border px-4 py-2"
                >We encourage the authors to propose more diverse and critical thinking, which can be adopted in various
                research areas regarding AI agents.
              </td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2"
                >At an appropriate level of difficulty, transferability and generalizability.</td
              >
              <td class="border px-4 py-2"
                >We hope that the proposal does not only focus on a toy project. Instead, we encourage the authors to
                propose a more insightful and constructive work, which may indicate that newly proposed ideas or
                solutions are usually more difficult than existing counterparts, but inherently obtain more
                transferability and generalizability against more various realistic scenarios.
              </td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2">Whether this work is able to be conducted or not.</td>
              <td class="border px-4 py-2"
                >We hope that the proposed ideas and methods are practical to implement and produce. Hence if there
                exists some preliminary experimental results, we recommend demonstrating them in your submission.
              </td>
            </tr>
          </tbody>
        </table>
      </div>
      <br />
      <h1 class="text-center font-bold text-4xl">Example Format of Proposal Submission</h1>
      <br />
      <p class="font-bold text-2xl">
        Recommended Format
        <p></p>
        <p>No more than 5 pages with ICML latex template (excluding reference and appendix).</p>
        <p class="font-bold text-2xl">Detailed Example Format</p>
        <p class="font-bold">I. Introduction (Proposal Description and Motivation)</p>
        <p>
          - In the introduction section, the most significant thing is to clarify the motivation of your research
          proposal for trustworthy agents. For example, which phenomenon or observation motivates you to propose this
          proposal. And what is the intuition behind it?
        </p>
        <p>
          - Then summarize the description of your proposal. Please Provide a brief, concrete description of the
          proposal regarding trustworthy AI agents. Is it an environment, dataset, or a detailed application of
          trustworthy agents? What are the inputs and outputs of your system in the proposal? And what is the goal of
          your proposal?
        </p>
        <p>
          - Besides, we recommend the authors to explain how your proposal could impact the research direction of the AI
          agent community regarding trustworthy agents. In example ideas, we have prepared several categories of
          research that generally help to assess or reduce the risks. Nevertheless, submissions will be judged according
          to their relevance to risks from AI agents, not limited to these categories.
        </p>
        <p class="font-bold">II. Related Works (previous attempts regarding AI agents and trustworthy AI)</p>
        <p>
          - In this section, the authors could review previous attempts regarding both AI agents and trustworthy
          foundation models (e.g, attack, defense, monitoring, and governance). Then explain how your proposal (a system
          or a benchmark) is similar to or different from previous counterparts. Good research proposals often tie into
          existing work to gain widespread adoption while inspiring novel future research.
        </p>
        <p class="font-bold">III. Technical Details</p>
        <p>
          - In this section, the authors should illustrate all the implementation details for your proposal, and prove
          that the proposal is practical and can be conducted.
        </p>
        <p>
          - If your proposal focuses on a synthetic environment / platform / playground for future trustworthy research,
          we recommend to demonstrate the following (but not limit to) details: the numbers and diversity of supporting
          scenarios,
        </p>
        <p>
          - If your proposal focuses on a trustworthy benchmark (maybe newly constructed or based on an existing agent
          environment), we recommend to illustrate comprehensive analysis (e.g., leaderboard) for existing agent
          solutions / applications.
        </p>
        <p>
          - If your proposal focuses on a detailed trustworthy AI agent framework, we recommend to state the baseline,
          the improvement methods and corresponding implementation details.
        </p>
        <p>
          - All the other related topics are also encouraged. Similar to previous examples, we require the authors to
          provide corresponding technical details.
        </p>
        <p class="font-bold">IV. Preliminary or Major Experimental Results (Optional, if available)</p>
        <p>
          - If you have tried your proposed proposals for trustworthy agents, we strongly encourage you to show the key
          preliminary or major results of your proposed agent environment / benchmark / solution.
        </p>
        <p>
          - Both quantitative results and qualitative results are encouraged to demonstrate. For trustworthy AI agent
          applications / solutions, one can provide quantitative results as comparison to show the effectiveness of
          proposed methods. For trustworthy agent benchmarks, one can show some quantitative results for existing
          baseline methods. And for trustworthy agent environments, one can illustrate critical qualitative results to
          show the effectiveness of the proposed environments.
        </p>
        <p class="font-bold">V.Conclusion and Relevance to Future Work</p>
        <p>
          - Current tractability analysis. Benchmarks should currently or soon be tractable for existing models while
          posing a meaningful challenge. A significant research effort should be required to achieve near-maximum
          performance.
        </p>
        <p>
          - Performance ceiling. Provide an estimate of maximum performance. For example, what would expert human-level
          performance be? Is it possible to achieve superhuman performance?
        </p>
        <p>
          - Barriers to entry. List factors that might make it harder for researchers to use your benchmark. Keep in
          mind that if barriers-to-entry trade-off is against relevance, you should generally prioritize the latter.
        </p>
        <p>1.How large do models need to be to perform well?</p>
        <p>2.How much context is required to understand the task?</p>
        <p>3.How difficult is it to adapt current training architectures to the dataset?</p>
        <p>
          4.Is third-party software (e.g. games, modeling software, simulators) or a complex set of dependencies
          required for training and/or evaluation?
        </p>
        <p>5.Is unusual hardware required (e.g. robotics, multi-GPU training setups)?</p>
        <p>
          6.Do researchers need to learn a new program or programming language to use the dataset (e.g., Coq, AnyLogic)?
        </p>
        <p class="font-bold">VI. Reference</p>
        <p>- as usual.</p>
      </p>
      <br />
    </Fragment>
  </Content>

  <FAQs
    title="Frequently Asked Questions"
    items={[
      {
        title: 'Will the submission be public?',
        description: 'Yes. After acceptance, your submission will be public as a workshop paper.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'What information should I include in the paper?',
        description: 'See the topic and example format section.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'Can I participate in a team?',
        description:
          '	Of course. We encourage researchers to collaborate in this work. For winning teams,  prizes will be divided evenly among the lead authors unless requested otherwise.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'May I submit a paper that has been published in conferences or journals?',
        description: 'No.',
        icon: 'tabler:help-octagon',
      },
    ]}
  />
</Layout>

<style></style>
