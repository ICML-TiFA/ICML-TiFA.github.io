---
import Header from '~/components/widgets/Header.astro';
import Hero2 from '~/components/widgets/Hero2.astro';
import Content from '~/components/widgets/Content.astro';
// import Timeline from '~/components/ui/Timeline.astro';
import { headerData } from '~/navigation';
import FAQs from '~/components/widgets/FAQs.astro';
import Brands from '~/components/widgets/Brands.astro';
// import Timezone from '~/components/widgets/Timezone.astro';
import Layout from '~/layouts/PageLayout.astro';
import Testimonials from '~/components/widgets/Testimonials.astro';
// import Sponsor from '~/components/widgets/Sponsor.astro';

const metadata = {
  title: 'Home',
};
---

<Layout metadata={metadata}>
  <Fragment slot="header">
    <Header {...headerData} isSticky />
  </Fragment>

  <!-- Hero2 Widget ******************* -->

  <Hero2
    tagline="ICML 2024 Workshop"
    actions={[{ variant: 'primary', text: 'Get Started', href: 'https://icml.cc/virtual/2024/workshop/29951' }]}
  >
    <Fragment slot="title">Trustworthy Multi-modal Foundation Models and AI Agents (TiFA)</Fragment>

    <Fragment slot="subtitle">
      The Forty-first International Conference on Machine Learning @ Messe Wien Exhibition Congress Center, Vienna,
      Austria
      <span class="text-slate-300 flex">Sun 21 Jul —— Sat 27 Jul</span>
    </Fragment>
  </Hero2>

  <!-- Content Widget **************** -->

  <!-- <Content id="Overview">
    <Fragment slot="title"> Overview </Fragment><Fragment slot="subtitle"
      >Welcome to the ICML 2024 Workshop on Trustworthy in Multi-modal Foundation Models and AI Agents (TiFA)
    </Fragment>
    <Fragment slot="content"
      ><div class="mb-4">
        Multi-modal Foundation Models (MFMs) have witnessed significant advancements in various bench marks and
        practical applications. They are proficient in a wide range of tasks, and capable of pro ducing varied
        multimodal responses. Nowadays a couple of MFMs have been deployed as realistic applications (such as GPT-4 [18]
        and Midjourney [25]). Meanwhile, a large variety of MFMs (like InternLM [24] and LLaVA [14]) are still under
        further research. Furthermore, foundation models with their powerful reasoning capabilities have led to the
        emergence of various AI Agents [19, 28]. These agents are often capable of understanding open-world
        instructions, breaking down complex tasks, and taking steps to achieve their goals.
      </div>
      <div class="mb-4">
        Advanced MFMs and AI Agents, equipped with diverse modalities and an increasing number of available affordances,
        accelerate their potential impact on society. As these systems gain abili ties to alter societal dynamics
        swiftly, understanding and preempting the vulnerabilities of such systems and their induced harms becomes
        crucial. Trustworthiness in MLMs and AI Agents tran scends identifying vulnerabilities in models and emphasizes
        the importance of proactive harm mit igation, safeguards, and the establishment of comprehensive safety
        mechanisms throughout the lifecycle of system development and deployment. This approach demands a blend of
        technical and socio-technical strategies, incorporating AI governance and regulatory insights to build
        trustworthy MFMs and Agents.
      </div>
    </Fragment>
    <Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment>
  </Content> -->

  <Content id="Call-For-Papers">
    <Fragment slot="title">Description/Call For Papers</Fragment>
    <Fragment slot="content"
      ><div class="mb-4">
        Advanced Multi-modal Foundation Models (MFMs) and AI Agents, equipped with diverse modalities [15, 26, 19, 25,
        29] and an increasing number of available affordances [20, 31] (e.g., tool use, code interpreter, API access,
        etc.), have the potential to accelerate and amplify their predecessors’ impact on society [9].
      </div>
      <div class="mb-4">
        MFM includes multi-modal large language models (MLLMs) and multi-modal generative models (MMGMs). <a
          href="https://arxiv.org/abs/2306.13549"
          class="hover:underline text-sky-600 font-medium"
          target="_blank">MLLMs</a
        > refer to LLM-based models with the ability to receive, reason, and output with information of multiple modalities,
        including but not limited to text, images, audio, and video. Examples include <a
          href="https://publications.reka.ai/reka-core-tech-report.pdf"
          class="hover:underline text-sky-600 font-medium"
          target="_blank">Reka</a
        >, QwenVL [24], Llava [25], and so on. MMGMs refer to a class of MFM models that can generate new content across
        multiple modalities, such as generating images from text descriptions or creating videos from audio and text
        inputs. Examples include Stable Diffusion [26], Sora, and Latte [27]. AI agents, <a
          href="https://cdn.openai.com/papers/practices-for-governing-agentic-ai-systems.pdf"
          class="hover:underline text-sky-600 font-medium"
          target="_blank">or systems with higher degree of agenticness</a
        >, refer to systems that could achieve complex goals in complex environments with limited direct supervision.
        Understanding and preempting the vulnerabilities of such systems [4] and their induced harms [6] becomes
        unprecedentedly crucial.
      </div>
      <div class="mb-4">
        Building trustworthy MFMs and AI Agents transcends adversarial robustness of such models, but also emphasizes
        the importance of proactive risk assessment, mitigation, safeguards, and the establishment of comprehensive
        safety mechanisms throughout the lifecycle of the systems’ development and deployment [24, 28]. This approach
        demands a blend of technical and socio-technical strategies, incorporating AI governance and regulatory insights
        to build trustworthy MFMs and AI Agents.
      </div>
      <div class="mb-4">Topics include but are not limited to:</div>
      <ul class="list-outside list-disc">
        <li>Adversarial robustness, attack and defense, poisoning, hijacking and security [8, 1, 4, 13, 22, 27]</li>
        <li>Out-of-distribution generalization, robustness to spurious correlations and uncertainty estimation</li>
        <li>Technical approaches to privacy, fairness, accountability and regulation [6, 18]</li>
        <li>Truthfulness, factuality, honesty and sycophancy [12, 23]</li>
        <li>Transparency, interpretability and monitoring [16, 32]</li>
        <li>Identifiers of AI-generated material, such as watermarking [14, 17]</li>
        <li>
          Alignment,<a
            href="https://www.aisafetybook.com/textbook/3-4"
            class="hover:underline text-sky-600 font-medium"
            target="_blank">control and machine ethics</a
          > [3, 21, 30]
        </li>
        <li>Model auditing, red-teaming and safety evaluation [5, 2, 10, 24, 7]</li>
        <li>Measures against malicious model fine-tuning [11]</li>
        <li>Novel safety challenges with the introduction of new modalities</li>
      </ul>
    </Fragment>
  </Content>

  <Content id="Submission-Guide" classes={{ container: 'pb-2 md:pb-2 lg:pb-2' }}>
    <Fragment slot="title">Submission Guide</Fragment>
    <Fragment slot="subtitle">Submission Instructions</Fragment>
    <Fragment slot="content" classes={{ container: 'py-2' }}
      ><ul class="list-outside list-disc">
        <li>Submissions should be made on OpenReview.</li>
        <li>
          Submissions should be anonymised papers up to 5 pages (appendices can be added to the main PDF); excluding
          references. Authors are required to include a "Social Impacts Statement" that highlights "potential broader
          impact of their work, including its ethical aspects and future societal consequences". You must format your
          submission using the <a
            href="https://media.icml.cc/Conferences/ICML2024/Styles/icml2024.zip"
            class="hover:underline text-sky-600 font-medium"
            target="_blank">ICML_LATEX_style_file</a
          >. Reviews will be double-blind, with at least two reviewers assigned to each paper.
        </li>
        <li>
          We welcome various types of papers including scientific papers and position papers. Scientific papers must not
          have appeared at an archival venue before, However, non-scientific papers that have appeared in archival
          venues outside main machine learning venues are welcomed for submission.
        </li>
        <li>
          All accepted papers will be available on the workshop website, but no formal workshop proceedings will be
          published.
        </li>
      </ul>
    </Fragment>
    <Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment>
  </Content>
  <Content classes={{ container: 'py-2 md:py-2 lg:py-2 pb-12 md:pb-12 lg:pb-12' }}>
    <Fragment slot="subtitle">Key Dates</Fragment>
    <Fragment slot="content" classes={{ container: 'p-0' }}>
      <!-- <Timeline
        items={[
          { title: 'May 06, 2024', description: 'Submissions Open' },
          { title: 'May 23, 2024', description: 'Submission Deadline' },
          { title: 'June 12, 2024', description: 'Acceptance Notification' },
          { title: 'June 30, 2024', description: 'Camera-Ready Deadline' },
          { title: 'July 26, 2024', description: 'Workshop Date' },
        ]}
        defaultIcon="tabler:arrow-right"
      /> -->
      <div class="flex justify-center">
        <table class="bg-white">
          <tbody>
            <tr>
              <td class="w-64 border px-4 py-2">Submissions Open</td>
              <td class="w-64 border px-4 py-2">May 06, 2024</td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2">Submission Deadline</td>
              <td class="border px-4 py-2">May 23, 2024</td>
            </tr>
            <tr>
              <td class="border px-4 py-2">Acceptance Notification</td>
              <td class="border px-4 py-2">June 12, 2024</td>
            </tr>
            <tr class="bg-gray-50">
              <td class="border px-4 py-2">Camera-Ready Deadline</td>
              <td class="border px-4 py-2">June 30, 2024</td>
            </tr>
            <tr>
              <td class="border px-4 py-2">Workshop Date</td>
              <td class="border px-4 py-2">July 26, 2024</td>
            </tr>
          </tbody>
        </table>
      </div>
      <div class="flex py-4">
        All deadlines are specified in <a
          href="https://www.timeanddate.com/time/zones/aoe"
          class="hover:underline text-sky-600 font-medium px-2"
          target="_blank">AoE</a
        > (Anywhere on Earth).
      </div>
    </Fragment>
    <Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment>
  </Content>
  <!-- Content Widget **************** -->

  <!-- <Content id="Schedule">
    <Fragment slot="content">
      <h3 class="text-3xl font-bold tracking-tight dark:text-white sm:text-4xl mb-2 text-center">
        Schedule<br />
      </h3>
      <Timezone
        items={[
          {
            time: '2024-05-15T08:50:00',
            theme: {
              title: 'Opening Remarks (10 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '#',
          },
          {
            time: '2024-05-15T09:00:00',
            theme: {
              title: 'Invited talk 1: Roger Grosse (30 min)',
            },
            Speaker: 'Roger Grosse',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T09:30:00',
            theme: {
              title: 'Invited talk 2: Matthias Hein (30 min)',
            },
            Speaker: 'Matthias Hein',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T10:00:00',
            theme: {
              title: 'Contributed Opinion Talk 1 (30 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T10:30:00',
            theme: {
              title: 'Break (15 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T10:45:00',
            theme: {
              title: 'Best Paper Talk 1 (15 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T11:00:00',
            theme: {
              title: 'Best Paper Talk 2 (15 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T11:15:00',
            theme: {
              title: 'Poster session & Lunch (110 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T13:05:00',
            theme: {
              title: 'Invited talk 3(30 min)',
            },
            Speaker: 'Been Kim',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T13:35:00',
            theme: {
              title: 'Invited talk 4(30 min)',
            },
            Speaker: 'Ludwig Schmidt',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T14:05:00',
            theme: {
              title: 'Contributed Opinion Talk 2(30 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T14:35:00',
            theme: {
              title: 'Invited talk 5(30 min)',
            },
            Speaker: 'Florian Tram‘er',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T15:05:00',
            theme: {
              title: 'Break (15 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T15:20:00',
            theme: {
              title: 'Invited talk 6(30 min)',
            },
            Speaker: 'Ivan Evtimov',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T15:50:00',
            theme: {
              title: 'Panel (60 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T16:50:00',
            theme: {
              title: 'Breakout rooms discussion (30 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
          {
            time: '2024-05-15T17:20:00',
            theme: {
              title: 'Closing remarks (10 min)',
            },
            Speaker: '',
            institution: '',
            logoSrc: '/avatar.png',
            link: '',
          },
        ]}
      />
    </Fragment>
  </Content> -->

  <!-- Brands Widget ****************** -->

  <!-- <Brands
    id="Speakers"
    title="Speakers"
    images={[
      {
        src: '/speaker01.png',
        alt: 'speaker01',
        name: 'Roger Grosse',
        university: 'University of Toronto',
        bioLink: 'https://www.cs.toronto.edu/~rgrosse/',
      },
      {
        src: '/speaker02.png',
        alt: 'speaker02',
        name: 'Been Kim',
        university: 'Google DeepMind',
        bioLink: 'https://beenkim.github.io/',
      },
      {
        src: '/speaker03.png',
        alt: 'speaker03',
        name: 'Florian Tramèr',
        university: 'ETH Zürich',
        bioLink: 'https://www.floriantramer.com/',
      },
      {
        src: '/speaker04.png',
        alt: 'speaker04',
        name: 'Ludwig Schmidt',
        university: 'University of Washington',
        bioLink: 'https://people.csail.mit.edu/ludwigs/',
      },
      {
        src: '/speaker05.png',
        alt: 'speaker05',
        name: 'Matthias Hein',
        university: 'University of Tübingen',
        bioLink: 'http://bit.ly/3UAht4W',
      },
      {
        src: '/speaker06.png',
        alt: 'speaker06',
        name: 'Ivan Evtimov',
        university: 'Meta',
        bioLink: 'https://ivanevtimov.eu/',
      },
    ]}
  >
    <Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment></Brands
  > -->
  <!-- Content Widget **************** -->

  <Content id="Challenges" title="Challenges">
    <Fragment slot="content"
      ><Testimonials
        classes={{ container: 'py-0 md:py-0 lg:py-0 ' }}
        testimonials={[
          {
            title: 'Track 1',
            name: 'OpenLane Topology Challenge',
            image: {
              src: 'https://images.unsplash.com/photo-1714244322811-f1387dc93909?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',
              alt: 'Sidney Hansen Image',
            },
            link: '/challenges#TRACK1',
            job: 'Go beyond conventional lane line detection as segmentation. Recognizing lanes as an abstraction of the scene - centerline, and building the topology between lanes and traffic elements. Such a topology is to facilitate planning and routing.',
          },
          {
            title: 'Track 2',
            name: 'OpenLane Topology Challenge',
            image: {
              src: 'https://images.unsplash.com/photo-1714244322811-f1387dc93909?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',
              alt: 'Sidney Hansen Image',
            },
            link: '/challenges#TRACK2',
            job: 'Go beyond conventional lane line detection as segmentation. Recognizing lanes as an abstraction of the scene - centerline, and building the topology between lanes and traffic elements. Such a topology is to facilitate planning and routing.',
          },
          {
            title: 'Track 3',
            name: 'OpenLane Topology Challenge',
            image: {
              src: 'https://images.unsplash.com/photo-1714244322811-f1387dc93909?q=80&w=2670&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D',
              alt: 'Sidney Hansen Image',
            },
            link: '/challenges#TRACK3',
            job: 'Go beyond conventional lane line detection as segmentation. Recognizing lanes as an abstraction of the scene - centerline, and building the topology between lanes and traffic elements. Such a topology is to facilitate planning and routing.',
          },
        ]}
      /></Fragment
    >
  </Content>
  <!-- Brands Widget ****************** -->

  <Brands
    id="Organizers"
    title="Organizers"
    icons={[]}
    images={[
      {
        src: '/organizer01.png',
        alt: 'organizer01',
        name: 'Zhenfei Yin',
        university: 'University of Sydney',
        bioLink: 'https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN',
      },
      {
        src: '/organizer02.png',
        alt: 'organizer02',
        name: 'Yawen Duan',
        university: 'Concordia AI',
        bioLink: 'https://scholar.google.com/citations?user=IJQlPvYAAAAJ&hl=en',
      },
      {
        src: '/organizer03.png',
        alt: 'organizer03',
        name: 'Bo Li',
        university: 'University of Illinois, Urbana Champaign',
        bioLink: 'https://aisecure.github.io/',
      },
      {
        src: '/organizer04.png',
        alt: 'organizer04',
        name: 'Hang Su',
        university: 'Tsinghua University',
        bioLink: 'https://www.suhangss.me/',
      },
      {
        src: '/organizer07.png',
        alt: 'organizer07',
        name: 'Pavel Izmailov',
        university: 'New York University',
        bioLink: 'https://izmailovpavel.github.io/',
      },
      {
        src: '/organizer05.png',
        alt: 'organizer05',
        name: 'Jianfeng Chi',
        university: 'Meta AI',
        bioLink: 'https://jfchi.github.io/',
      },
      {
        src: '/organizer11.jpg',
        alt: 'organizer11',
        name: 'Lijun Li',
        university: 'Shanghai AI Lab',
        bioLink: 'https://scholar.google.com/citations?user=394j5K4AAAAJ&hl=zh-CN',
      },
      {
        src: '/organizer09.png',
        alt: 'organizer09',
        name: 'Andy Zou',
        university: 'Carnegie Mellon University',
        bioLink: 'https://andyzoujm.github.io/',
      },
      {
        src: '/organizer08.png',
        alt: 'organizer08',
        name: 'Neil Gong',
        university: 'Duke University',
        bioLink: 'https://people.duke.edu/~zg70/',
      },
      {
        src: '/organizer10.png',
        alt: 'organizer10',
        name: 'Yaodong Yang',
        university: 'Peking University',
        bioLink: 'https://www.yangyaodong.com/',
      },
      {
        src: '/organizer06.png',
        alt: 'organizer06',
        name: 'Peyman Najafirad',
        university: 'University of Texas, San Antonio',
        bioLink: 'https://scholar.google.com/citations?user=uoCn8c8AAAAJ&hl=en',
      },
    ]}
    ><Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment></Brands
  >
  <!-- Brands Widget ****************** -->

  <!-- <Brands
    id="Organizers"
    title=""
    classes={{ container: 'lg:py-1 md:py-1' }}
    tagline="Challenge Chair"
    icons={[]}
    images={[
      {
        src: '/organizer01.png',
        alt: 'organizer01',
        name: 'Zhenfei Yin',
        university: 'University of Sydney',
        bioLink: 'https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN',
      },
      {
        src: '/organizer02.png',
        alt: 'organizer02',
        name: 'Yawen Duan',
        university: 'University of Cambridge',
        bioLink: 'https://scholar.google.com/citations?user=IJQlPvYAAAAJ&hl=en',
      },
      {
        src: '/organizer03.png',
        alt: 'organizer03',
        name: 'Jianfeng Chi',
        university: 'Meta AI',
        bioLink: 'https://jfchi.github.io/',
      },
    ]}
  /> -->
  <!-- Brands Widget ****************** -->

  <!-- <Brands
    classes={{ container: 'lg:py-1 md:py-1 lg:pb-20 md:pb-16' }}
    id="Organizers"
    tagline="program committee"
    icons={[]}
    images={[
      {
        src: '/organizer01.png',
        alt: 'organizer01',
        name: 'Zhenfei Yin',
        university: 'University of Sydney',
        bioLink: 'https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN',
      },
      {
        src: '/organizer02.png',
        alt: 'organizer02',
        name: 'Yawen Duan',
        university: 'University of Cambridge',
        bioLink: 'https://scholar.google.com/citations?user=IJQlPvYAAAAJ&hl=en',
      },
      {
        src: '/organizer03.png',
        alt: 'organizer03',
        name: 'Jianfeng Chi',
        university: 'Meta AI',
        bioLink: 'https://jfchi.github.io/',
      },
      {
        src: '/organizer04.png',
        alt: 'organizer04',
        name: 'Hang Su',
        university: 'Tsinghua University',
        bioLink: 'https://www.suhangss.me/',
      },
      {
        src: '',
        alt: 'organizer06',
        name: 'Jing Shao',
        university: 'Shanghai AI Laboratory',
        bioLink: 'https://amandajshao.github.io/',
      },
      {
        src: '',
        alt: 'organizer07',
        name: 'Pavel Izmailov',
        university: 'OpenAI',
        bioLink: 'https://izmailovpavel.github.io/',
      },
      {
        src: '',
        alt: 'organizer08',
        name: 'Zhengyi Wang',
        university: 'Tsinghua University',
        bioLink: 'https://thuwzy.github.io/',
      },
      {
        src: '',
        alt: 'organizer09',
        name: 'Andy Zou',
        university: 'Carnegie Mellon University',
        bioLink: 'https://andyzoujm.github.io/',
      },
    ]}
  /> -->

  <!-- Brands Widget ****************** -->

  <Brands
    id="Steering-Committee"
    title="Steering Committee"
    icons={[]}
    images={[
      {
        src: '/committee01.png',
        alt: 'committee01',
        name: 'Jing Shao',
        university: 'Shanghai AI Lab',
        bioLink: 'https://amandajshao.github.io/',
      },
      {
        src: '/committee02.png',
        alt: 'committee02',
        name: 'Yu Qiao',
        university: 'Shanghai AI Lab',
        bioLink: 'https://mmlab.siat.ac.cn/yuqiao',
      },
      {
        src: '/committee03.png',
        alt: 'committee03',
        name: 'Jun Zhu',
        university: 'Tsinghua University',
        bioLink: 'https://ml.cs.tsinghua.edu.cn/~jun/index.shtml',
      },
      {
        src: '/committee04.png',
        alt: 'committee04',
        name: 'Xuanjing Huang',
        university: 'Fudan University',
        bioLink: 'https://xuanjing-huang.github.io/',
      },
      {
        src: '/committee05.png',
        alt: 'committee05',
        name: 'Alan Yuille',
        university: 'Johns Hopkins University',
        bioLink: 'https://www.cs.jhu.edu/~ayuille/',
      },
      {
        src: '/committee06.png',
        alt: 'committee06',
        name: 'Wanli Ouyang',
        university: 'Shanghai AI Lab',
        bioLink: 'https://wlouyang.github.io/',
      },
      {
        src: '/committee07.png',
        alt: 'committee07',
        name: 'Dacheng Tao',
        university: 'Nanyang Technological University',
        bioLink: 'https://ieeexplore.ieee.org/author/37269935500',
      },
      {
        src: '/committee08.png',
        alt: 'committee08',
        name: 'Philip Torr',
        university: 'University of Oxford',
        bioLink: 'https://www.robots.ox.ac.uk/~phst/',
      },
    ]}
  />
  <!-- FAQs Widget ******************* -->

  <FAQs
    title="Frequently Asked Questions"
    items={[
      {
        title: 'Can we submit a paper that will also be submitted to NeurIPS 2024?',
        description: 'Yes.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'Can we submit a paper that was accepted at ICLR 2024?',
        description: 'No. ICML prohibits main conference publication from appearing concurrently at the workshops.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'Will the reviews be made available to authors?',
        description: 'Yes.',
        icon: 'tabler:help-octagon',
      },
      {
        title: 'I have a question not addressed here, whom should I contact?',
        description: 'Email organizers at icml-tifa-workshop@googlegroups.com',
        icon: 'tabler:help-octagon',
      },
    ]}
    ><Fragment slot="bg">
      <div class="absolute inset-0 bg-blue-50 dark:bg-transparent"></div>
    </Fragment></FAQs
  >

  <Content id="References">
    <Fragment slot="title"> References </Fragment>

    <Fragment slot="content">
      <p class="text-slate-500">
        [1] L. Bailey, E. Ong, S. Russell, and S. Emmons. Image hijacks: Adversarial images can control generative
        models at runtime, 2023.
      </p>
      <p class="text-slate-500">
        [2] M. Bhatt, S. Chennabasappa, C. Nikolaidis, S. Wan, I. Evtimov, D. Gabi, D. Song, F. Ahmad, C. Aschermann, L.
        Fontana, S. Frolov, R. P. Giri, D. Kapil, Y. Kozyrakis, D. LeBlanc, J. Mi lazzo, A. Straumann, G. Synnaeve, V.
        Vontimitta, S. Whitman, and J. Saxe. Purple llama cyberseceval: A secure coding benchmark for language models,
        2023.
      </p>
      <p class="text-slate-500">
        [3] S. R. Bowman, J. Hyun, E. Perez, E. Chen, C. Pettit, S. Heiner, K. Lukoˇsi ̄ut ̇e, A. Askell, A. Jones, A.
        Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Olah, D. Amodei, D. Amodei, D. Drain, D. Li, E. Tran-Johnson, J.
        Kernion, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, L. Lovitt, N. Elhage, N. Schiefer, N. Joseph, N.
        Mercado, N. DasSarma, R. Larson, S. McCandlish, S. Kundu, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T.
        Telleen-Lawton, T. Brown, T. Henighan, T. Hume, Y. Bai, Z. Hatfield-Dodds, B. Mann, and J. Kaplan. Measuring
        progress on scalable oversight for large language models, 2022.
      </p>
      <p class="text-slate-500">
        [4] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, A. Awadalla, P. W. Koh, D. Ippolito, K.
        Lee, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially aligned?, 2023.
      </p>
      <p class="text-slate-500">
        [5] S. Casper, C. Ezell, C. Siegmann, N. Kolt, T. L. Curtis, B. Bucknall, A. Haupt, K. Wei, J. Scheurer, M.
        Hobbhahn, L. Sharkey, S. Krishna, M. V. Hagen, S. Alberti, A. Chan, Q. Sun, M. Gerovitch, D. Bau, M. Tegmark, D.
        Krueger, and D. Hadfield-Menell. Black-box access is insufficient for rigorous ai audits, 2024.
      </p>
      <p class="text-slate-500">
        [6] A. Chan, R. Salganik, A. Markelius, C. Pang, N. Rajkumar, D. Krasheninnikov, L. Langosco, Z. He, Y. Duan, M.
        Carroll, M. Lin, A. Mayhew, K. Collins, M. Molamohammadi, J. Burden, W. Zhao, S. Rismani, K. Voudouris, U.
        Bhatt, A. Weller, D. Krueger, and T. Maharaj. Harms from increasingly agentic algorithmic systems. In 2023 ACM
        Conference on Fairness, Accountability, and Transparency, FAccT ’23. ACM, June 2023. doi:
        10.1145/3593013.3594033. URL http://dx.doi.org/10.1145/3593013.3594033.
      </p>
      <p class="text-slate-500">
        [7] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y.
        Chang, P. S. Yu, Q. Yang, and X. Xie. A survey on evaluation of large language models, 2023.
      </p>
      <p class="text-slate-500">
        [8] Y. Dong, H. Chen, J. Chen, Z. Fang, X. Yang, Y. Zhang, Y. Tian, H. Su, and J. Zhu. How robust is google’s
        bard to adversarial image attacks?, 2023.
      </p>
      <p class="text-slate-500">
        [9] T. Eloundou, S. Manning, P. Mishkin, and D. Rock. Gpts are gpts: An early look at the labor market impact
        potential of large language models, 2023.
      </p>
      <p class="text-slate-500">
        [10] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer, K.
        Ndousse, A. Jones, S. Bowman, A. Chen, T. Conerly, N. DasSarma, D. Drain, N. Elhage, S. El-Showk, S. Fort, Z.
        Hatfield-Dodds, T. Henighan, D. Hernandez, T. Hume, J. Jacobson, S. Johnston, S. Kravec, C. Olsson, S. Ringer,
        E. Tran-Johnson, D. Amodei, T. Brown, N. Joseph, S. McCandlish, C. Olah, J. Kaplan, and J. Clark. Red teaming
        language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.
      </p>
      <p class="text-slate-500">
        [11] Henderson, P., Mitchell, E., Manning, C., Jurafsky, D., & Finn, C. (2023). Self-destructing models:
        Increasing the costs of harmful dual uses of foundation models. In Proceedings of the 2023 AAAI/ACM Conference
        on AI, Ethics, and Society, AIES ’23.
      </p>
      <p class="text-slate-500">
        [12] Huang, Q., Dong, X., Zhang, P., Wang, B., He, C., Wang, J., Lin, D., Zhang, W., & Yu, N. (2023). Opera:
        Alleviating hallucination in multi-modal large language models via over-trust penalty and
        retrospection-allocation.
      </p>
      <p class="text-slate-500">
        [13] Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., yeh Chiang, P., ... & Goldstein, T.
        (2023). Baseline defenses for adversarial attacks against aligned language models.
      </p>
      <p class="text-slate-500">
        [14] Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., & Goldstein, T. (2023). A watermark for large
        language models. In Proceedings of the 40th International Conference on Machine Learning.
      </p>
      <p class="text-slate-500">[15] Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2023). Visual instruction tuning.</p>
      <p class="text-slate-500">
        [16] Meng, K., Bau, D., Andonian, A., & Belinkov, Y. (2022). Locating and editing factual associations in GPT.
      </p>
      <p class="text-slate-500">
        [17] Nasr, M., Carlini, N., Hayase, J., Jagielski, M., Cooper, A. F., Ippolito, D., ... & Lee, K. (2023).
        Scalable extraction of training data from (production) language models.
      </p>
      <p class="text-slate-500">[18] OpenAI. (2023). Practices for governing agentic AI systems.</p>
      <p class="text-slate-500">[19] OpenAI. (2023). GPT-4 with vision (GPT-4v) system card.</p>
      <p class="text-slate-500">
        [20] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., ... & Sun, M. (2023). ToolIIM: Facilitating large
        language models to master 16000+ real-world APIs.
      </p>
      <p class="text-slate-500">
        [21] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct preference
        optimization: Your language model is secretly a reward model.
      </p>
      <p class="text-slate-500">
        [22] Robey, A., Wong, E., Hassani, H., & Pappas, G. J. (2023). SmoothLLM: Defending large language models
        against jailbreaking attacks.
      </p>
      <p class="text-slate-500">
        [23] Sharma, M., Tong, M., Korbak, T., Duvenaud, D., Askell, A., Bowman, S. R., ... & Perez, E. (2023). Towards
        understanding sycophancy in language models.
      </p>
      <p class="text-slate-500">
        [24] Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., ... & Zhou, J. (2023). Qwen-vl: A frontier large
        vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966.
      </p>
      <p class="text-slate-500">
        [25] Liu, H., Li, C., Wu, Q., & Lee, Y. J. (2024). Visual instruction tuning. Advances in neural information
        processing systems, 36.
      </p>
      <p class="text-slate-500">
        [26] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with
        latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
      </p>
      <p class="text-slate-500">
        [27] Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y. F., ... & Qiao, Y. (2024). Latte: Latent diffusion
        transformer for video generation. arXiv preprint arXiv:2401.03048.
      </p>
    </Fragment>
  </Content>

  <!-- Brands Widget ****************** -->
  <!-- 
  <Sponsor
    id="Sponsor"
    tagline="Sponsor"
    icons={[]}
    images={[
      {
        src: 'https://cdn.pixabay.com/photo/2015/05/26/09/37/paypal-784404_1280.png',
        alt: 'Paypal',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2021/12/06/13/48/visa-6850402_1280.png',
        alt: 'Visa',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2013/10/01/10/29/ebay-189064_1280.png',
        alt: 'Ebay',
      },

      {
        src: 'https://cdn.pixabay.com/photo/2015/04/13/17/45/icon-720944_1280.png',
        alt: 'Youtube',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2013/02/12/09/07/microsoft-80658_1280.png',
        alt: 'Microsoft',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2015/04/23/17/41/node-js-736399_1280.png',
        alt: 'Node JS',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2015/10/31/12/54/google-1015751_1280.png',
        alt: 'Google',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2021/12/06/13/45/meta-6850393_1280.png',
        alt: 'Meta',
      },
      {
        src: 'https://cdn.pixabay.com/photo/2013/01/29/22/53/yahoo-76684_1280.png',
        alt: 'Yahoo',
      },
    ]}
  /> -->
</Layout>
